{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install datasets torch transformers sentencepiece ipywidgets protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
      "    num_rows: 2270\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
      "    num_rows: 2281\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\")\n",
    "print(dataset['validation'])\n",
    "print(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> using cuda\n",
      "> loading MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\n",
      "> loading MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"> using {device}\")\n",
    "MODELS = {}\n",
    "TOKENIZERS = {}\n",
    "\n",
    "model_name_base = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "model_name_large = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n",
    "model_name_large_2 = \"Joelzhang/deberta-v3-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "\n",
    "for model_name in [model_name_base, model_name_large]:\n",
    "    print(f\"> loading {model_name}\")\n",
    "    TOKENIZERS[model_name] = AutoTokenizer.from_pretrained(model_name)\n",
    "    MODELS[model_name] = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model_name:str, premise:str, hypothesis:str):\n",
    "    tokenizer = TOKENIZERS[model_name]\n",
    "    model = MODELS[model_name]\n",
    "    \n",
    "    tokenizer_out = tokenizer(premise, hypothesis, truncation=False, return_tensors='pt')\n",
    "    model_out = model(tokenizer_out['input_ids'].to(device))\n",
    "    prediction = torch.softmax(model_out[\"logits\"][0], -1).tolist()\n",
    "    label_names = [\"ENTAILMENT\", \"NEUTRAL\", \"CONTRADICTION\"]\n",
    "    prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "    return max(prediction, key=prediction.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "> processing test data: 100%|██████████| 2281/2281 [04:12<00:00,  9.04it/s]\n",
      "> processing validation data: 100%|██████████| 2270/2270 [03:02<00:00, 12.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3144"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "first_round = {\n",
    "    'cid':[], \n",
    "    'premise':[], \n",
    "    'hypothesis': [], \n",
    "    'label': []\n",
    "}\n",
    "\n",
    "for split in ['test', 'validation']:\n",
    "    for sample in tqdm(dataset[split], desc=f\"> processing {split} data\"):\n",
    "        premise = sample['premise']\n",
    "        hypothesis = sample['hypothesis']\n",
    "        label = sample['label']\n",
    "        \n",
    "        if premise == '' or hypothesis == '':\n",
    "            continue\n",
    "\n",
    "        valid_sample_flag = True\n",
    "        for model_name in MODELS.keys():\n",
    "            predicted_label = inference(model_name, premise, hypothesis)\n",
    "            valid_sample_flag = (predicted_label == label)\n",
    "            #print(f\"> sample {sample['id']} predicted {predicted_label} --- gold {label} >>> {valid_sample_flag}\")\n",
    "            if not valid_sample_flag:  \n",
    "                break\n",
    "        \n",
    "        if valid_sample_flag:\n",
    "            first_round['cid'].append(sample['id'])\n",
    "            first_round['premise'].append(premise)\n",
    "            first_round['hypothesis'].append(hypothesis)\n",
    "            first_round['label'].append(label)\n",
    "\n",
    "first_round = pl.from_dict(first_round)\n",
    "first_round.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_round.write_csv('fever_validation_filtered_first_round.csv', separator=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same but with the third model (all three models do not fit in my GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> loading Joelzhang/deberta-v3-large-snli_mnli_fever_anli_R1_R2_R3-nli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukfre/miniconda3/envs/nlp2024/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODELS = {}\n",
    "TOKENIZERS = {}\n",
    "\n",
    "for model_name in [model_name_large_2]:\n",
    "    print(f\"> loading {model_name}\")\n",
    "    TOKENIZERS[model_name] = AutoTokenizer.from_pretrained(model_name)\n",
    "    MODELS[model_name] = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "> processing first round: 100%|██████████| 3144/3144 [03:09<00:00, 16.59it/s]\n"
     ]
    }
   ],
   "source": [
    "second_round = {\n",
    "    'id':[], \n",
    "    'cid':[], \n",
    "    'premise':[], \n",
    "    'hypothesis': [], \n",
    "    'label': []\n",
    "}\n",
    "\n",
    "\n",
    "i = 0\n",
    "for sample in tqdm(first_round.iter_rows(named=True), desc=\"> processing first round\", total=first_round.height):\n",
    "    premise = sample['premise']\n",
    "    hypothesis = sample['hypothesis']\n",
    "    label = sample['label']\n",
    "    \n",
    "    valid_sample_flag = True\n",
    "    for model_name in MODELS.keys():\n",
    "        predicted_label = inference(model_name, premise, hypothesis)\n",
    "        valid_sample_flag = (predicted_label == label)\n",
    "        #print(f\"> sample {sample['cid']} predicted {predicted_label} --- gold {label} >>> {valid_sample_flag}\")\n",
    "        if not valid_sample_flag:  \n",
    "            break\n",
    "    \n",
    "    if valid_sample_flag:\n",
    "        second_round['id'].append(i)\n",
    "        second_round['cid'].append(sample['cid'])\n",
    "        second_round['premise'].append(premise)\n",
    "        second_round['hypothesis'].append(hypothesis)\n",
    "        second_round['label'].append(label)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3076 elements (-68)\n"
     ]
    }
   ],
   "source": [
    "second_round = pl.from_dict(second_round)\n",
    "print(f\"{second_round.height} elements (-{first_round.height - second_round.height})\")\n",
    "second_round.write_csv('fever_validation_filtered_second_round.csv', separator=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 5)\n",
      "┌──────┬────────┬─────────────────────────────────┬─────────────────────────────────┬────────────┐\n",
      "│ id   ┆ cid    ┆ premise                         ┆ hypothesis                      ┆ label      │\n",
      "│ ---  ┆ ---    ┆ ---                             ┆ ---                             ┆ ---        │\n",
      "│ i64  ┆ str    ┆ str                             ┆ str                             ┆ str        │\n",
      "╞══════╪════════╪═════════════════════════════════╪═════════════════════════════════╪════════════╡\n",
      "│ 520  ┆ 201374 ┆ Varsity Blues (film) . The fil… ┆ Varsity Blues (film) was filme… ┆ ENTAILMENT │\n",
      "│ 957  ┆ 109431 ┆ Visigoths . In or around 589 ,… ┆ The culture of their Hispano-R… ┆ ENTAILMENT │\n",
      "│ 922  ┆ 70935  ┆ China . , it is the world 's s… ┆ The world's second largest eco… ┆ ENTAILMENT │\n",
      "│ 1667 ┆ 179029 ┆ Steve Ditko . Ditko studied un… ┆ Steve Ditko studied at school.  ┆ ENTAILMENT │\n",
      "│ 1004 ┆ 135477 ┆ Google Search . These include … ┆ Scores for sports games can be… ┆ ENTAILMENT │\n",
      "└──────┴────────┴─────────────────────────────────┴─────────────────────────────────┴────────────┘\n",
      "shape: (5, 5)\n",
      "┌──────┬────────┬─────────────────────────────────┬─────────────────────────────────┬─────────┐\n",
      "│ id   ┆ cid    ┆ premise                         ┆ hypothesis                      ┆ label   │\n",
      "│ ---  ┆ ---    ┆ ---                             ┆ ---                             ┆ ---     │\n",
      "│ i64  ┆ str    ┆ str                             ┆ str                             ┆ str     │\n",
      "╞══════╪════════╪═════════════════════════════════╪═════════════════════════════════╪═════════╡\n",
      "│ 481  ┆ 212777 ┆ Harvard University . The under… ┆ The undergraduate college of H… ┆ NEUTRAL │\n",
      "│ 882  ┆ 151020 ┆ Rory William Quigley ( born Ja… ┆ Harry Fraud graduated high sch… ┆ NEUTRAL │\n",
      "│ 1700 ┆ 206731 ┆ Samwell Tarly , called Sam , i… ┆ Samwell Tarly is killed in Gam… ┆ NEUTRAL │\n",
      "│ 965  ┆ 1773   ┆ Sabbir Khan . In 2009 he made … ┆ Sabbir Khan directed a film st… ┆ NEUTRAL │\n",
      "│ 1398 ┆ 57892  ┆ Pierce County, Washington . Pi… ┆ Pierce County, Washington is m… ┆ NEUTRAL │\n",
      "└──────┴────────┴─────────────────────────────────┴─────────────────────────────────┴─────────┘\n",
      "shape: (5, 5)\n",
      "┌──────┬────────┬─────────────────────────────────┬────────────────────────────────┬───────────────┐\n",
      "│ id   ┆ cid    ┆ premise                         ┆ hypothesis                     ┆ label         │\n",
      "│ ---  ┆ ---    ┆ ---                             ┆ ---                            ┆ ---           │\n",
      "│ i64  ┆ str    ┆ str                             ┆ str                            ┆ str           │\n",
      "╞══════╪════════╪═════════════════════════════════╪════════════════════════════════╪═══════════════╡\n",
      "│ 939  ┆ 42817  ┆ Musala ( Мусала from Arabic th… ┆ Musala is the highest peak in  ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ …                              ┆               │\n",
      "│ 968  ┆ 112933 ┆ Salman Rushdie . His second no… ┆ Salman Rushdie started         ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ writing…                       ┆               │\n",
      "│ 1387 ┆ 120422 ┆ Temple Grandin is a 2010 biopi… ┆ Temple Grandin was directed    ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ by…                            ┆               │\n",
      "│ 44   ┆ 117065 ┆ Renato Balestra . Born in Trie… ┆ Renato Balestra was born       ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ outsi…                         ┆               │\n",
      "│ 701  ┆ 97783  ┆ Lithuanians '' ' ( lietuviai ,… ┆ Lithuanians are alien to       ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ Lithu…                         ┆               │\n",
      "└──────┴────────┴─────────────────────────────────┴────────────────────────────────┴───────────────┘\n",
      "shape: (150, 5)\n",
      "┌──────┬────────┬─────────────────────────────────┬────────────────────────────────┬───────────────┐\n",
      "│ id   ┆ cid    ┆ premise                         ┆ hypothesis                     ┆ label         │\n",
      "│ ---  ┆ ---    ┆ ---                             ┆ ---                            ┆ ---           │\n",
      "│ i64  ┆ str    ┆ str                             ┆ str                            ┆ str           │\n",
      "╞══════╪════════╪═════════════════════════════════╪════════════════════════════════╪═══════════════╡\n",
      "│ 184  ┆ 207516 ┆ The X Factor is a British tele… ┆ Mel B collaborated with Ciara. ┆ NEUTRAL       │\n",
      "│ 1728 ┆ 155868 ┆ Renato Balestra . Born in Trie… ┆ Renato Balestra came from an   ┆ ENTAILMENT    │\n",
      "│      ┆        ┆                                 ┆ e…                             ┆               │\n",
      "│ 845  ┆ 183423 ┆ Simi Valley, California . The … ┆ In 2012, Simi Valley,          ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ Californ…                      ┆               │\n",
      "│ 1934 ┆ 65691  ┆ Highway to Heaven is an Americ… ┆ Highway to Heaven is           ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ something…                     ┆               │\n",
      "│ 1653 ┆ 186596 ┆ Asylum Records is an American … ┆ Asylum Records is an American  ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ …                              ┆               │\n",
      "│ …    ┆ …      ┆ …                               ┆ …                              ┆ …             │\n",
      "│ 922  ┆ 70935  ┆ China . , it is the world 's s… ┆ The world's second largest     ┆ ENTAILMENT    │\n",
      "│      ┆        ┆                                 ┆ eco…                           ┆               │\n",
      "│ 1134 ┆ 172708 ┆ Northwestern University ( NU )… ┆ Northwestern University is     ┆ NEUTRAL       │\n",
      "│      ┆        ┆                                 ┆ the…                           ┆               │\n",
      "│ 2904 ┆ 27073  ┆ See You on the Other Side (Kor… ┆ See You on the Other Side was  ┆ ENTAILMENT    │\n",
      "│      ┆        ┆                                 ┆ …                              ┆               │\n",
      "│ 2628 ┆ 192584 ┆ Live Nation Entertainment is a… ┆ Live Nation Entertainment      ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ form…                          ┆               │\n",
      "│ 2961 ┆ 8576   ┆ Dodo . It has been depicted wi… ┆ The Dodo has been depicted     ┆ ENTAILMENT    │\n",
      "│      ┆        ┆                                 ┆ wit…                           ┆               │\n",
      "└──────┴────────┴─────────────────────────────────┴────────────────────────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "sampled_e = second_round.filter(pl.col('label') == 'ENTAILMENT').sample(n=50, seed=42)\n",
    "sampled_n = second_round.filter(pl.col('label') == 'NEUTRAL').sample(n=50, seed=42)\n",
    "sampled_c = second_round.filter(pl.col('label') == 'CONTRADICTION').sample(n=50, seed=42)\n",
    "print(sampled_e.head())\n",
    "print(sampled_n.head())\n",
    "print(sampled_c.head())\n",
    "concat = pl.concat([sampled_e, sampled_n, sampled_c]).select(pl.all().shuffle(seed=42))\n",
    "print(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv written.\n",
      "shape: (3, 2)\n",
      "┌───────────────┬─────┐\n",
      "│ label         ┆ len │\n",
      "│ ---           ┆ --- │\n",
      "│ str           ┆ u32 │\n",
      "╞═══════════════╪═════╡\n",
      "│ NEUTRAL       ┆ 50  │\n",
      "│ ENTAILMENT    ┆ 50  │\n",
      "│ CONTRADICTION ┆ 50  │\n",
      "└───────────────┴─────┘\n"
     ]
    }
   ],
   "source": [
    "to_csv = {\n",
    "    'id' : [],\n",
    "    'cid' : [], \n",
    "    'premise': [],\n",
    "    'hypothesis' : [],\n",
    "    'alternative hypothesis' : [], \n",
    "    'label' : [],\t\n",
    "    'new hypothesis': [], \t\n",
    "    'new label': [], \n",
    "    'change type':[]\n",
    "}\n",
    "for i, sample in enumerate(concat.iter_rows(named=True)):\n",
    "    to_csv['id'].append(i)\n",
    "    to_csv['cid'].append(sample['cid'])\n",
    "    to_csv['premise'].append(sample['premise'])\n",
    "    to_csv['hypothesis'].append(sample['hypothesis'])\n",
    "    to_csv['alternative hypothesis'].append('')\n",
    "    to_csv['label'].append(sample['label'])\n",
    "    to_csv['new hypothesis'].append('')\n",
    "    to_csv['new label'].append('')\n",
    "    to_csv['change type'].append('')\n",
    "to_csv = pl.from_dict(to_csv)\n",
    "to_csv.write_csv(\"sampled_fever_validation_filtered.csv\", separator=',')\n",
    "print(\"csv written.\")\n",
    "\n",
    "q = (\n",
    "    to_csv.lazy()\n",
    "    .group_by(\"label\")\n",
    "    .len()\n",
    ")\n",
    "df = q.collect()\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
