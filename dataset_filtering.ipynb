{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install datasets torch transformers sentencepiece ipywidgets protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3250c0ba97c54bb88103936822b738f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84071b032d9c4e0db4c1e04a801d5975",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/71.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b2be6611fc42cd88ae98a033b14ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e35d27ad4447ffa7f0725ae226f0f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/3.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9965542ed6c4002a9a6b2653893b300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/51086 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfaacbc087104470a14afa889a4e66c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecabe764eff74c73be62647e8d13509a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2287 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
      "    num_rows: 2288\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'premise', 'hypothesis', 'label', 'wsd', 'srl'],\n",
      "    num_rows: 2287\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\", download_mode=\"force_redownload\")\n",
    "print(dataset['validation'])\n",
    "print(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> using cuda\n",
      "> loading MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\n",
      "> loading MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(f\"> using {device}\")\n",
    "MODELS = {}\n",
    "TOKENIZERS = {}\n",
    "\n",
    "model_name_base = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "model_name_large = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n",
    "model_name_large_2 = \"Joelzhang/deberta-v3-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "\n",
    "for model_name in [model_name_base, model_name_large]:\n",
    "    print(f\"> loading {model_name}\")\n",
    "    TOKENIZERS[model_name] = AutoTokenizer.from_pretrained(model_name)\n",
    "    MODELS[model_name] = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model_name:str, premise:str, hypothesis:str):\n",
    "    tokenizer = TOKENIZERS[model_name]\n",
    "    model = MODELS[model_name]\n",
    "    \n",
    "    tokenizer_out = tokenizer(premise, hypothesis, truncation=False, return_tensors='pt')\n",
    "    model_out = model(tokenizer_out['input_ids'].to(device))\n",
    "    prediction = torch.softmax(model_out[\"logits\"][0], -1).tolist()\n",
    "    label_names = [\"ENTAILMENT\", \"NEUTRAL\", \"CONTRADICTION\"]\n",
    "    prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
    "    return max(prediction, key=prediction.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "> processing test data: 100%|██████████| 2287/2287 [02:58<00:00, 12.81it/s]\n",
      "> processing validation data: 100%|██████████| 2288/2288 [03:06<00:00, 12.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3453"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "first_round = {\n",
    "    'cid':[], \n",
    "    'premise':[], \n",
    "    'hypothesis': [], \n",
    "    'label': []\n",
    "}\n",
    "\n",
    "for split in ['test', 'validation']:\n",
    "    for sample in tqdm(dataset[split], desc=f\"> processing {split} data\"):\n",
    "        premise = sample['premise']\n",
    "        hypothesis = sample['hypothesis']\n",
    "        label = sample['label']\n",
    "        \n",
    "        if premise == '' or hypothesis == '':\n",
    "            continue\n",
    "\n",
    "        valid_sample_flag = True\n",
    "        for model_name in MODELS.keys():\n",
    "            predicted_label = inference(model_name, premise, hypothesis)\n",
    "            valid_sample_flag = (predicted_label == label)\n",
    "            #print(f\"> sample {sample['id']} predicted {predicted_label} --- gold {label} >>> {valid_sample_flag}\")\n",
    "            if not valid_sample_flag:  \n",
    "                break\n",
    "        \n",
    "        if valid_sample_flag:\n",
    "            first_round['cid'].append(sample['id'])\n",
    "            first_round['premise'].append(premise)\n",
    "            first_round['hypothesis'].append(hypothesis)\n",
    "            first_round['label'].append(label)\n",
    "\n",
    "first_round = pl.from_dict(first_round)\n",
    "first_round.height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_round.write_csv('fever_validation_filtered_first_round.csv', separator=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same but with the third model (all three models do not fit in my GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> loading Joelzhang/deberta-v3-large-snli_mnli_fever_anli_R1_R2_R3-nli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukfre/miniconda3/envs/nlp2024/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "MODELS = {}\n",
    "TOKENIZERS = {}\n",
    "\n",
    "for model_name in [model_name_large_2]:\n",
    "    print(f\"> loading {model_name}\")\n",
    "    TOKENIZERS[model_name] = AutoTokenizer.from_pretrained(model_name)\n",
    "    MODELS[model_name] = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "> processing first round: 100%|██████████| 3453/3453 [03:39<00:00, 15.70it/s]\n"
     ]
    }
   ],
   "source": [
    "second_round = {\n",
    "    'id':[], \n",
    "    'cid':[], \n",
    "    'premise':[], \n",
    "    'hypothesis': [], \n",
    "    'label': []\n",
    "}\n",
    "\n",
    "\n",
    "i = 0\n",
    "for sample in tqdm(first_round.iter_rows(named=True), desc=\"> processing first round\", total=first_round.height):\n",
    "    premise = sample['premise']\n",
    "    hypothesis = sample['hypothesis']\n",
    "    label = sample['label']\n",
    "    \n",
    "    valid_sample_flag = True\n",
    "    for model_name in MODELS.keys():\n",
    "        predicted_label = inference(model_name, premise, hypothesis)\n",
    "        valid_sample_flag = (predicted_label == label)\n",
    "        #print(f\"> sample {sample['cid']} predicted {predicted_label} --- gold {label} >>> {valid_sample_flag}\")\n",
    "        if not valid_sample_flag:  \n",
    "            break\n",
    "    \n",
    "    if valid_sample_flag:\n",
    "        second_round['id'].append(i)\n",
    "        second_round['cid'].append(sample['cid'])\n",
    "        second_round['premise'].append(premise)\n",
    "        second_round['hypothesis'].append(hypothesis)\n",
    "        second_round['label'].append(label)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3361 elements (-92)\n"
     ]
    }
   ],
   "source": [
    "second_round = pl.from_dict(second_round)\n",
    "print(f\"{second_round.height} elements (-{first_round.height - second_round.height})\")\n",
    "second_round.write_csv('fever_validation_filtered_second_round.csv', separator=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 5)\n",
      "┌──────┬────────┬─────────────────────────────────┬─────────────────────────────────┬────────────┐\n",
      "│ id   ┆ cid    ┆ premise                         ┆ hypothesis                      ┆ label      │\n",
      "│ ---  ┆ ---    ┆ ---                             ┆ ---                             ┆ ---        │\n",
      "│ i64  ┆ str    ┆ str                             ┆ str                             ┆ str        │\n",
      "╞══════╪════════╪═════════════════════════════════╪═════════════════════════════════╪════════════╡\n",
      "│ 528  ┆ 71246  ┆ Appropriation (art) . The use … ┆ Appropriation (art) played a s… ┆ ENTAILMENT │\n",
      "│ 1076 ┆ 12612  ┆ The Challenge XXX : Dirty 30 i… ┆ The Challenge XXX: Dirty 30 is… ┆ ENTAILMENT │\n",
      "│ 1039 ┆ 199669 ┆ Andrea Pirlo , Ufficiale OMRI … ┆ Andrea Pirlo plays for two tea… ┆ ENTAILMENT │\n",
      "│ 2485 ┆ 26804  ┆ Byron Howard . He was nominate… ┆ A Golden Globe was won by Byro… ┆ ENTAILMENT │\n",
      "│ 1851 ┆ 48819  ┆ The Lincoln -- Douglas Debates… ┆ The Lincoln-Douglas debates oc… ┆ ENTAILMENT │\n",
      "└──────┴────────┴─────────────────────────────────┴─────────────────────────────────┴────────────┘\n",
      "shape: (5, 5)\n",
      "┌──────┬────────┬─────────────────────────────────┬─────────────────────────────────┬─────────┐\n",
      "│ id   ┆ cid    ┆ premise                         ┆ hypothesis                      ┆ label   │\n",
      "│ ---  ┆ ---    ┆ ---                             ┆ ---                             ┆ ---     │\n",
      "│ i64  ┆ str    ┆ str                             ┆ str                             ┆ str     │\n",
      "╞══════╪════════╪═════════════════════════════════╪═════════════════════════════════╪═════════╡\n",
      "│ 517  ┆ 32422  ┆ Indian Army . It is an all-vol… ┆ The Indian Army comprises more… ┆ NEUTRAL │\n",
      "│ 934  ┆ 214984 ┆ Sensitive Skin (Canadian TV se… ┆ Sensitive Skin's first series … ┆ NEUTRAL │\n",
      "│ 2414 ┆ 214252 ┆ David Marvin Blake ( born Janu… ┆ DJ Quik's dog's name is David … ┆ NEUTRAL │\n",
      "│ 1772 ┆ 154698 ┆ Damon Albarn . His debut solo … ┆ Damon Albarn married Brian Eno… ┆ NEUTRAL │\n",
      "│ 510  ┆ 228328 ┆ Island Records . It was founde… ┆ Island Records was reviewed by… ┆ NEUTRAL │\n",
      "└──────┴────────┴─────────────────────────────────┴─────────────────────────────────┴─────────┘\n",
      "shape: (5, 5)\n",
      "┌──────┬────────┬─────────────────────────────────┬────────────────────────────────┬───────────────┐\n",
      "│ id   ┆ cid    ┆ premise                         ┆ hypothesis                     ┆ label         │\n",
      "│ ---  ┆ ---    ┆ ---                             ┆ ---                            ┆ ---           │\n",
      "│ i64  ┆ str    ┆ str                             ┆ str                            ┆ str           │\n",
      "╞══════╪════════╪═════════════════════════════════╪════════════════════════════════╪═══════════════╡\n",
      "│ 1808 ┆ 208429 ┆ Excuse My French is the debut … ┆ Excuse My French is an album   ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ b…                             ┆               │\n",
      "│ 539  ┆ 65017  ┆ Aristotle . At seventeen or ei… ┆ Aristotle never went to the    ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ Ac…                            ┆               │\n",
      "│ 811  ┆ 53177  ┆ Aleister Crowley ( [ ˈkroʊli ]… ┆ Aleister Crowley died on       ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ Octob…                         ┆               │\n",
      "│ 851  ┆ 134850 ┆ Ice-T . He began his career as… ┆ Ice-T refused to ever make     ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ hip…                           ┆               │\n",
      "│ 1327 ┆ 225273 ┆ Brian Russell De Palma ( born … ┆ Brian De Palma was born in     ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ Jan…                           ┆               │\n",
      "└──────┴────────┴─────────────────────────────────┴────────────────────────────────┴───────────────┘\n",
      "shape: (150, 5)\n",
      "┌──────┬────────┬─────────────────────────────────┬────────────────────────────────┬───────────────┐\n",
      "│ id   ┆ cid    ┆ premise                         ┆ hypothesis                     ┆ label         │\n",
      "│ ---  ┆ ---    ┆ ---                             ┆ ---                            ┆ ---           │\n",
      "│ i64  ┆ str    ┆ str                             ┆ str                            ┆ str           │\n",
      "╞══════╪════════╪═════════════════════════════════╪════════════════════════════════╪═══════════════╡\n",
      "│ 526  ┆ 58846  ┆ Johnny Galecki . He is known f… ┆ Johnny Galecki has been in at  ┆ NEUTRAL       │\n",
      "│      ┆        ┆                                 ┆ …                              ┆               │\n",
      "│ 813  ┆ 172460 ┆ Matteo Renzi ( [ matˈtɛːo ˈrɛn… ┆ Matteo Renzi served as Prime   ┆ ENTAILMENT    │\n",
      "│      ┆        ┆                                 ┆ M…                             ┆               │\n",
      "│ 2513 ┆ 181201 ┆ Southpaw is a 2015 American sp… ┆ Southpaw was released 2011.    ┆ CONTRADICTION │\n",
      "│ 1819 ┆ 174024 ┆ Pink Floyd . The final Pink Fl… ┆ The Endless River is Pink      ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ Floy…                          ┆               │\n",
      "│ 2134 ┆ 118068 ┆ Beatles Day . This day is cons… ┆ Liverpool is unrelated to The  ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ …                              ┆               │\n",
      "│ …    ┆ …      ┆ …                               ┆ …                              ┆ …             │\n",
      "│ 1039 ┆ 199669 ┆ Andrea Pirlo , Ufficiale OMRI … ┆ Andrea Pirlo plays for two     ┆ ENTAILMENT    │\n",
      "│      ┆        ┆                                 ┆ tea…                           ┆               │\n",
      "│ 3049 ┆ 109821 ┆ Rizwan Ahmed ( [ ; born 1 Dece… ┆ Riz Ahmed is an award          ┆ NEUTRAL       │\n",
      "│      ┆        ┆                                 ┆ nominate…                      ┆               │\n",
      "│ 1062 ┆ 156831 ┆ The Cordilleran ice sheet was … ┆ Cordilleran Ice Sheet was      ┆ ENTAILMENT    │\n",
      "│      ┆        ┆                                 ┆ loca…                          ┆               │\n",
      "│ 931  ┆ 119221 ┆ Zoey Francis Thompson Deutch (… ┆ Zoey Deutch is an American     ┆ CONTRADICTION │\n",
      "│      ┆        ┆                                 ┆ fro…                           ┆               │\n",
      "│ 2342 ┆ 170946 ┆ Smriti Mandhana ( born 18 July… ┆ Smriti Mandhana is an Indian   ┆ ENTAILMENT    │\n",
      "│      ┆        ┆                                 ┆ w…                             ┆               │\n",
      "└──────┴────────┴─────────────────────────────────┴────────────────────────────────┴───────────────┘\n"
     ]
    }
   ],
   "source": [
    "sampled_e = second_round.filter(pl.col('label') == 'ENTAILMENT').sample(n=50, seed=42)\n",
    "sampled_n = second_round.filter(pl.col('label') == 'NEUTRAL').sample(n=50, seed=42)\n",
    "sampled_c = second_round.filter(pl.col('label') == 'CONTRADICTION').sample(n=50, seed=42)\n",
    "print(sampled_e.head())\n",
    "print(sampled_n.head())\n",
    "print(sampled_c.head())\n",
    "concat = pl.concat([sampled_e, sampled_n, sampled_c]).select(pl.all().shuffle(seed=42))\n",
    "print(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv written.\n",
      "shape: (3, 2)\n",
      "┌───────────────┬─────┐\n",
      "│ label         ┆ len │\n",
      "│ ---           ┆ --- │\n",
      "│ str           ┆ u32 │\n",
      "╞═══════════════╪═════╡\n",
      "│ NEUTRAL       ┆ 50  │\n",
      "│ ENTAILMENT    ┆ 50  │\n",
      "│ CONTRADICTION ┆ 50  │\n",
      "└───────────────┴─────┘\n"
     ]
    }
   ],
   "source": [
    "to_csv = {\n",
    "    'id' : [],\n",
    "    'cid' : [], \n",
    "    'premise': [],\n",
    "    'hypothesis' : [],\n",
    "    'alternative hypothesis' : [], \n",
    "    'label' : [],\t\n",
    "    'new hypothesis': [], \t\n",
    "    'new label': [], \n",
    "    'change type':[]\n",
    "}\n",
    "for i, sample in enumerate(concat.iter_rows(named=True)):\n",
    "    to_csv['id'].append(i)\n",
    "    to_csv['cid'].append(sample['cid'])\n",
    "    to_csv['premise'].append(sample['premise'])\n",
    "    to_csv['hypothesis'].append(sample['hypothesis'])\n",
    "    to_csv['alternative hypothesis'].append('')\n",
    "    to_csv['label'].append(sample['label'])\n",
    "    to_csv['new hypothesis'].append('')\n",
    "    to_csv['new label'].append('')\n",
    "    to_csv['change type'].append('')\n",
    "to_csv = pl.from_dict(to_csv)\n",
    "to_csv.write_csv(\"sampled_fever_validation_filtered.csv\", separator=',')\n",
    "print(\"csv written.\")\n",
    "\n",
    "q = (\n",
    "    to_csv.lazy()\n",
    "    .group_by(\"label\")\n",
    "    .len()\n",
    ")\n",
    "df = q.collect()\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
