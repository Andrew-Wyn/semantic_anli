{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>ID</th><th>CID</th><th>PREMISE</th><th>HYPOTHESIS</th><th>ALTERNATIVE HYPOTHESIS (only if you&#x27;re not using the original one)</th><th>LABEL</th><th>NEW HYPHOTESIS</th><th>NEW LABEL</th><th>CHANGE TYPE</th><th>BASE MODEL</th><th>LARGE1 MODEL</th><th>LARGE2 MODEL</th><th>DIFFICULTY SCORE</th></tr><tr><td>i64</td><td>i64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>0</td><td>58846</td><td>&quot;Johnny Galecki . He is known f…</td><td>&quot;Johnny Galecki has been in at …</td><td>null</td><td>&quot;NEUTRAL&quot;</td><td>&quot;The number of sitcoms from Fra…</td><td>&quot;ENTAILMENT&quot;</td><td>&quot;math&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1</td><td>172460</td><td>&quot;Matteo Renzi ( [ matˈtɛːo ˈrɛn…</td><td>&quot;Matteo Renzi served as Prime M…</td><td>null</td><td>&quot;ENTAILMENT&quot;</td><td>&quot;Matteo Renzi was the president…</td><td>&quot;NEUTRAL&quot;</td><td>&quot;coreference&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>2</td><td>181201</td><td>&quot;Southpaw is a 2015 American sp…</td><td>&quot;Southpaw was released 2011.&quot;</td><td>null</td><td>&quot;CONTRADICTION&quot;</td><td>&quot;Among the movies that premiere…</td><td>&quot;ENTAILMENT&quot;</td><td>&quot;rephrasing, math&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>3</td><td>174024</td><td>&quot;Pink Floyd . The final Pink Fl…</td><td>&quot;The Endless River is Pink Floy…</td><td>null</td><td>&quot;CONTRADICTION&quot;</td><td>&quot;counting Pink Floyd&#x27;s albums f…</td><td>&quot;ENTAILMENT&quot;</td><td>&quot;ranking&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td></tr><tr><td>4</td><td>118068</td><td>&quot;Beatles Day . This day is cons…</td><td>&quot;Liverpool is unrelated to The …</td><td>&quot;Beatles Day was celebrated in …</td><td>&quot;CONTRADICTION&quot;</td><td>&quot;Beatles Day was celebrated 20 …</td><td>&quot;ENTAILMENT&quot;</td><td>&quot;math&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 13)\n",
       "┌─────┬────────┬─────────────────┬─────────────────┬───┬────────────┬────────┬────────┬────────────┐\n",
       "│ ID  ┆ CID    ┆ PREMISE         ┆ HYPOTHESIS      ┆ … ┆ BASE MODEL ┆ LARGE1 ┆ LARGE2 ┆ DIFFICULTY │\n",
       "│ --- ┆ ---    ┆ ---             ┆ ---             ┆   ┆ ---        ┆ MODEL  ┆ MODEL  ┆ SCORE      │\n",
       "│ i64 ┆ i64    ┆ str             ┆ str             ┆   ┆ str        ┆ ---    ┆ ---    ┆ ---        │\n",
       "│     ┆        ┆                 ┆                 ┆   ┆            ┆ str    ┆ str    ┆ str        │\n",
       "╞═════╪════════╪═════════════════╪═════════════════╪═══╪════════════╪════════╪════════╪════════════╡\n",
       "│ 0   ┆ 58846  ┆ Johnny Galecki  ┆ Johnny Galecki  ┆ … ┆ null       ┆ null   ┆ null   ┆ null       │\n",
       "│     ┆        ┆ . He is known   ┆ has been in at  ┆   ┆            ┆        ┆        ┆            │\n",
       "│     ┆        ┆ f…              ┆ …               ┆   ┆            ┆        ┆        ┆            │\n",
       "│ 1   ┆ 172460 ┆ Matteo Renzi (  ┆ Matteo Renzi    ┆ … ┆ null       ┆ null   ┆ null   ┆ null       │\n",
       "│     ┆        ┆ [ matˈtɛːo      ┆ served as Prime ┆   ┆            ┆        ┆        ┆            │\n",
       "│     ┆        ┆ ˈrɛn…           ┆ M…              ┆   ┆            ┆        ┆        ┆            │\n",
       "│ 2   ┆ 181201 ┆ Southpaw is a   ┆ Southpaw was    ┆ … ┆ null       ┆ null   ┆ null   ┆ null       │\n",
       "│     ┆        ┆ 2015 American   ┆ released 2011.  ┆   ┆            ┆        ┆        ┆            │\n",
       "│     ┆        ┆ sp…             ┆                 ┆   ┆            ┆        ┆        ┆            │\n",
       "│ 3   ┆ 174024 ┆ Pink Floyd .    ┆ The Endless     ┆ … ┆ null       ┆ null   ┆ null   ┆ null       │\n",
       "│     ┆        ┆ The final Pink  ┆ River is Pink   ┆   ┆            ┆        ┆        ┆            │\n",
       "│     ┆        ┆ Fl…             ┆ Floy…           ┆   ┆            ┆        ┆        ┆            │\n",
       "│ 4   ┆ 118068 ┆ Beatles Day .   ┆ Liverpool is    ┆ … ┆ null       ┆ null   ┆ null   ┆ null       │\n",
       "│     ┆        ┆ This day is     ┆ unrelated to    ┆   ┆            ┆        ┆        ┆            │\n",
       "│     ┆        ┆ cons…           ┆ The …           ┆   ┆            ┆        ┆        ┆            │\n",
       "└─────┴────────┴─────────────────┴─────────────────┴───┴────────────┴────────┴────────┴────────────┘"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "df = pl.read_csv(\"fever_test.adversarial.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> loading MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\n",
      "> loading MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\n",
      "> loading Joelzhang/deberta-v3-large-snli_mnli_fever_anli_R1_R2_R3-nli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lukfre/miniconda3/envs/nlp2024/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m> loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m     TOKENIZERS[model_name] \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m---> 20\u001b[0m     MODELS[model_name] \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minference\u001b[39m(model_name, premise, hypothesis):\n\u001b[1;32m     24\u001b[0m     model_input \u001b[38;5;241m=\u001b[39m TOKENIZERS[model_name](premise, hypothesis, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2024/lib/python3.11/site-packages/transformers/modeling_utils.py:2724\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2720\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2721\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2722\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2723\u001b[0m         )\n\u001b[0;32m-> 2724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2024/lib/python3.11/site-packages/torch/nn/modules/module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2024/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2024/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 779 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2024/lib/python3.11/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2024/lib/python3.11/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp2024/lib/python3.11/site-packages/torch/nn/modules/module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1154\u001b[0m             device,\n\u001b[1;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1156\u001b[0m             non_blocking,\n\u001b[1;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1158\u001b[0m         )\n\u001b[0;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "MODELS = {}\n",
    "TOKENIZERS = {}\n",
    "\n",
    "model_name_base = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
    "model_name_large = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n",
    "model_name_large_2 = \"Joelzhang/deberta-v3-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
    "MAP = {\n",
    "    'base' : model_name_base,\n",
    "    'large1': model_name_large,\n",
    "    'large2': model_name_large_2,\n",
    "}\n",
    "\n",
    "for model_name in MAP.values():\n",
    "    print(f\"> loading {model_name}\")\n",
    "    TOKENIZERS[model_name] = AutoTokenizer.from_pretrained(model_name)\n",
    "    MODELS[model_name] = AutoModelForSequenceClassification.from_pretrained(model_name).to(device)\n",
    "\n",
    "\n",
    "def inference(model_name, premise, hypothesis):\n",
    "    model_input = TOKENIZERS[model_name](premise, hypothesis, truncation=False, return_tensors=\"pt\")\n",
    "    output = MODELS[model_name](model_input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
    "    prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
    "    label_names = [\"ENTAILMENT\", \"NEUTRAL\", \"CONTRADICTION\"]\n",
    "    return {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,1,1\n",
      "0,0,0\n",
      "0,0,0\n",
      "0,0,1\n",
      "0,0,0\n",
      "0,1,0\n",
      "0,0,0\n",
      "0,1,0\n",
      "0,0,0\n",
      "1,1,1\n",
      "1,1,1\n",
      "1,0,0\n",
      "0,1,0\n",
      "0,0,0\n",
      "0,1,1\n",
      "0,1,1\n",
      "0,0,0\n",
      "0,1,1\n",
      "0,0,0\n",
      "0,0,0\n",
      "0,1,0\n",
      "0,0,0\n",
      "0,0,0\n",
      "0,0,1\n",
      "0,1,1\n",
      "0,1,0\n",
      "0,1,0\n",
      "1,1,1\n",
      "0,1,1\n",
      "0,1,1\n",
      "0,0,1\n"
     ]
    }
   ],
   "source": [
    "for i, elem in enumerate(df.iter_rows(named=True)):\n",
    "    result = {'base': None, 'large1': None, 'large2': None}\n",
    "    print()\n",
    "    print(f\"SAMPLE {elem['ID']} - LABEL: {elem['LABEL']}\")\n",
    "    for model_id, model_name in MAP.items(): \n",
    "        prediction = inference(model_name, elem['PREMISE'], elem['NEW HYPOTHESIS'])\n",
    "        print(prediction)\n",
    "        result[model_id] = max(prediction, key=prediction.get)\n",
    "    print(f\"{int(result['base']==elem['LABEL'])},{int(result['large1']==elem['LABEL'])},{int(result['large2']==elem['LABEL'])}\")\n",
    "    with open(\"scores.log\", 'w+') as fout:\n",
    "        print(f\"{int(result['base']==elem['LABEL'])},{int(result['large1']==elem['LABEL'])},{int(result['large2']==elem['LABEL'])}\",file=fout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
