{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Initialization\n",
        "---\n",
        "This cell downloads and extracts the dataset from https://www.dropbox.com/s/hylbuaovqwo2zav/nli_fever.zip.\n",
        "- Execute it **ONLY ONCE**, at the start of your work."
      ],
      "metadata": {
        "id": "ZkKypenUbeEV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/hylbuaovqwo2zav/nli_fever.zip\n",
        "!unzip \"nli_fever.zip\"\n",
        "!rm \"nli_fever.zip\"\n",
        "!rm -r \"__MACOSX\"\n",
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAjAFGW8bjoX",
        "outputId": "a17b722d-5005-48fc-a292-b2f263b5a5be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-22 13:17:59--  https://www.dropbox.com/s/hylbuaovqwo2zav/nli_fever.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6017:18::a27d:212\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/hylbuaovqwo2zav/nli_fever.zip [following]\n",
            "--2024-05-22 13:18:00--  https://www.dropbox.com/s/raw/hylbuaovqwo2zav/nli_fever.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc4c561a38b1012df90c4649a7ac.dl.dropboxusercontent.com/cd/0/inline/CTZihYlM0cIB6hGRkQiKsL6sifE_befAxSPSLTjbOmWfEk5IyJLjqZnxMHV-YHqs-wmW1DQi2ItVBl6ayq43cqvgsS8V0leBiFSTD3qxnzF0UPgh_dgf7rT2cbdY8aAf488IKamFbi9YIIW1hUtofLtS/file# [following]\n",
            "--2024-05-22 13:18:01--  https://uc4c561a38b1012df90c4649a7ac.dl.dropboxusercontent.com/cd/0/inline/CTZihYlM0cIB6hGRkQiKsL6sifE_befAxSPSLTjbOmWfEk5IyJLjqZnxMHV-YHqs-wmW1DQi2ItVBl6ayq43cqvgsS8V0leBiFSTD3qxnzF0UPgh_dgf7rT2cbdY8aAf488IKamFbi9YIIW1hUtofLtS/file\n",
            "Resolving uc4c561a38b1012df90c4649a7ac.dl.dropboxusercontent.com (uc4c561a38b1012df90c4649a7ac.dl.dropboxusercontent.com)... 162.125.2.15, 2620:100:6020:15::a27d:400f\n",
            "Connecting to uc4c561a38b1012df90c4649a7ac.dl.dropboxusercontent.com (uc4c561a38b1012df90c4649a7ac.dl.dropboxusercontent.com)|162.125.2.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CTYHH94j0xkauN_62UeK7skhI_stqO1hQV4PGIPfnYCA5yg9CoNs9ObYpZf-PipTssoMe4qQdKA00tTlouq03ql8ejS03RPeAdS4fMDEfdhKgtwYulkfKAyglDsPScxjFUdXhOgpuRx6i88IHeFTxfgx56JuQKZeJIDmar_rOohKGb_oyI6QcbVr-xugeTNQbncV-QpuHy_CSM0kxASz1QoJByo2GbEpACuUSPoVc4wiv4geYggf13JkSIcnaa3h3d66aCiDSXb7taic2FcVl2lLBvxGKn_PQ5qSx9PhcXNBULccSiC9zaLt-IOWZToF8-D8MW499WYJlY6VB9rU1Chdcn5Ng9GKM78HMZczaPcIW0uBLnqAUCpuzCMQ9eqNV_E/file [following]\n",
            "--2024-05-22 13:18:01--  https://uc4c561a38b1012df90c4649a7ac.dl.dropboxusercontent.com/cd/0/inline2/CTYHH94j0xkauN_62UeK7skhI_stqO1hQV4PGIPfnYCA5yg9CoNs9ObYpZf-PipTssoMe4qQdKA00tTlouq03ql8ejS03RPeAdS4fMDEfdhKgtwYulkfKAyglDsPScxjFUdXhOgpuRx6i88IHeFTxfgx56JuQKZeJIDmar_rOohKGb_oyI6QcbVr-xugeTNQbncV-QpuHy_CSM0kxASz1QoJByo2GbEpACuUSPoVc4wiv4geYggf13JkSIcnaa3h3d66aCiDSXb7taic2FcVl2lLBvxGKn_PQ5qSx9PhcXNBULccSiC9zaLt-IOWZToF8-D8MW499WYJlY6VB9rU1Chdcn5Ng9GKM78HMZczaPcIW0uBLnqAUCpuzCMQ9eqNV_E/file\n",
            "Reusing existing connection to uc4c561a38b1012df90c4649a7ac.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36923425 (35M) [application/zip]\n",
            "Saving to: ‘nli_fever.zip’\n",
            "\n",
            "nli_fever.zip       100%[===================>]  35.21M   103MB/s    in 0.3s    \n",
            "\n",
            "2024-05-22 13:18:02 (103 MB/s) - ‘nli_fever.zip’ saved [36923425/36923425]\n",
            "\n",
            "Archive:  nli_fever.zip\n",
            "replace nli_fever/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: a\n",
            "error:  invalid response [a]\n",
            "replace nli_fever/.DS_Store? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: nli_fever/.DS_Store     \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/nli_fever/\n",
            "  inflating: __MACOSX/nli_fever/._.DS_Store  \n",
            "  inflating: nli_fever/train_fitems.jsonl  \n",
            "  inflating: __MACOSX/nli_fever/._train_fitems.jsonl  \n",
            "  inflating: nli_fever/dev_fitems.jsonl  \n",
            "  inflating: __MACOSX/nli_fever/._dev_fitems.jsonl  \n",
            "  inflating: nli_fever/README.md     \n",
            "  inflating: nli_fever/test_fitems.jsonl  \n",
            "  inflating: __MACOSX/nli_fever/._test_fitems.jsonl  \n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 May 22 13:18 nli_fever\n",
            "drwxr-xr-x 1 root root 4096 May 20 13:25 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These cells initialize the models and the dataset.\n",
        "- You need to execute it **ONLY ONCE**, but, if for any reason the process crashes, you may try re-running from this cell (so you'll avoid downloading files again).\n",
        "- If it still crashes, then re-run from the start."
      ],
      "metadata": {
        "id": "I4j87xP4bo5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "random.seed(3983751073717997123)\n",
        "\n",
        "LABEL_MAP = {\n",
        "    'SUPPORTS': 'entailment',\n",
        "    'NOT ENOUGH INFO': 'neutral',\n",
        "    'REFUTES': 'contradiction'\n",
        "}\n",
        "TRAIN_PATH = 'nli_fever/train_fitems.jsonl'\n",
        "\n",
        "with open(TRAIN_PATH, 'r') as fin:\n",
        "    dataset = []\n",
        "    for line in fin:\n",
        "        dataset.append(json.loads(line))\n",
        "\n",
        "to_sample = random.sample(population=range(0, len(dataset)), k=100)\n",
        "sampled = [dataset[i] for i in to_sample]\n",
        "print(len(dataset), 'samples')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5Z79Qv2boh6",
        "outputId": "c1d8e35e-b48a-4450-fa0e-c4e51b64928e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "208346 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lp_nESuWf4X0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "def initialize_models():\n",
        "    models = {}\n",
        "    tokenizer = {}\n",
        "\n",
        "    model_name_base = \"MoritzLaurer/DeBERTa-v3-base-mnli-fever-anli\"\n",
        "    model_name_large = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n",
        "    model_name_large_2 = \"Joelzhang/deberta-v3-large-snli_mnli_fever_anli_R1_R2_R3-nli\"\n",
        "\n",
        "    tokenizer_base = AutoTokenizer.from_pretrained(model_name_base)\n",
        "    model_base = AutoModelForSequenceClassification.from_pretrained(model_name_base)\n",
        "\n",
        "    tokenizer_large = AutoTokenizer.from_pretrained(model_name_large)\n",
        "    model_large = AutoModelForSequenceClassification.from_pretrained(model_name_large)\n",
        "\n",
        "    tokenizer_large_2 = AutoTokenizer.from_pretrained(model_name_large_2)\n",
        "    model_large_2 = AutoModelForSequenceClassification.from_pretrained(model_name_large_2)\n",
        "\n",
        "\n",
        "    models = {\"base\": model_base.to(device),\n",
        "             \"large\": model_large.to(device),\n",
        "             \"large2\": model_large_2.to(device)}\n",
        "    tokenizers = {\"base\": tokenizer_base,\n",
        "             \"large\": tokenizer_large,\n",
        "             \"large2\": tokenizer_large_2}\n",
        "    return tokenizers, models\n",
        "\n",
        "\n",
        "def get_prediction(premise, hypothesis, model):\n",
        "    model_input = tokenizers[chosen_model](premise, hypothesis, truncation=False, return_tensors=\"pt\")\n",
        "    output = models[chosen_model](model_input[\"input_ids\"].to(device))  # device = \"cuda:0\" or \"cpu\"\n",
        "    prediction = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
        "    label_names = [\"entailment\", \"neutral\", \"contradiction\"]\n",
        "    prediction = {name: round(float(pred) * 100, 1) for pred, name in zip(prediction, label_names)}\n",
        "    return prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizers, models = initialize_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ-R68FrgUIw",
        "outputId": "edb946ef-d74b-4759-b57e-bf6fcddf5a65"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:560: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# The Main Loop\n",
        "This cell contains the main part of the program: it will loop through each sample of the dataset, asking you to provide a new, hard to understand, hypothesis for each of them.\n",
        "\n",
        "You can choose either to:\n",
        "1. modify the given hypothesis, keeping the same label\n",
        "2. come up with a new hypothesis and its correspective label (you can also use ChatGPT for ideas)\n",
        "\n",
        "In both cases, when writing the result on [this google sheet](https://docs.google.com/spreadsheets/d/1k7JTOOS2jUDItxCh7xSjwf3eGR8skGP7P7HQGh7_WCg/edit#gid=0), write also the main \"change\" you performed.\n",
        "- You can come up with your categorization or take inspiration from the one of [this paper](https://arxiv.org/pdf/2010.12729) (see Table 2).\n",
        "\n",
        "NOTE: **The changes on the hypothesis can be anything as long as the label does not change**."
      ],
      "metadata": {
        "id": "EQ_ZYLYqcJUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Formal Definition\n",
        "**Given**:\n",
        "- *M* :   ensemble of models that you will fool\n",
        "- *P* :   premise (the 'context')\n",
        "- *H* :   hypothesis (the 'claim'), simple enough so that *M* correctly classifies the relationship between *P* and *H*\n",
        "- *L* :   gold label (the relationship between *P* and *H*)\n",
        "\n",
        "**Task**: generate *H'* such that:\n",
        "1. *H* and *H'* have more or less the same meaning --> the relationship between *P* and *H'* is the same as the relationship between *P* and *H*\n",
        "2. *H'* can fool *M* --> *M* will predict a different relationship type"
      ],
      "metadata": {
        "id": "cUDJxaa1cRBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "last = int(input(\"If you are resuming, enter the last ID you worked on (otherwise 0): \"))\n",
        "assert last < len(dataset), f\"You entered an ID value that is higher than the size of the dataset -- Rerun this cell.\"\n",
        "\n",
        "i = max(0, last)\n",
        "for elem in sampled[last:]:\n",
        "    chosen_model = random.choice(list(models.keys()))\n",
        "    print(\"-\"*30)\n",
        "    print(f\"[ID {i} - CID {elem['cid']} - model to fool: {chosen_model}]\")\n",
        "    print(f\"PREMISE:\")\n",
        "    for context in elem['context'].split('.'):\n",
        "        if context.strip() != '':\n",
        "            print(f\"\\t> {context.strip()}.\")\n",
        "    print(f\"HYPOTHESIS:\\n\\t> {elem['query']}\")\n",
        "    print(f\"GOLD LABEL: {LABEL_MAP[elem['label']]}\")\n",
        "    print(\"-\"*30)\n",
        "\n",
        "    hypothesis = input(\"> type new hypothesis: \")\n",
        "    while hypothesis.lower() != 'n':\n",
        "        prediction = get_prediction(elem['context'], hypothesis, chosen_model)\n",
        "        # rescore for better visibility\n",
        "        #prediction = {k: int(v*100) for k, v in prediction.items()}\n",
        "        predicted = max(prediction, key=prediction.get)\n",
        "        if predicted != LABEL_MAP[elem[\"label\"]]:\n",
        "            print(f\"PREDICTED LABEL **CHANGED**: >>>> {predicted} <<<< -- {prediction}\", flush=True)\n",
        "        else:\n",
        "            print(f\"PREDICTED LABEL: {predicted} -- {prediction}\", flush=True)\n",
        "        hypothesis = input(\"type n to exit, otherwise type new hyphotesis: \")\n",
        "\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "LrNH7EJscYhT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}